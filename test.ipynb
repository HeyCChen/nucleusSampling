{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import scatter\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES', use_node_attr=True)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "for step, data in enumerate(loader):\n",
    "    edge_index, batch = data.edge_index, data.batch\n",
    "\n",
    "    x = torch.rand(batch.size(0))\n",
    "    # print(x)\n",
    "\n",
    "    num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n",
    "    print(num_nodes)\n",
    "    batch_size, max_num_nodes = num_nodes.size(0), int(num_nodes.max())\n",
    "\n",
    "    cum_num_nodes = torch.cat(\n",
    "        [num_nodes.new_zeros(1),\n",
    "         num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "    index = torch.arange(batch.size(0), dtype=torch.long)\n",
    "    # print(index)\n",
    "    # index - cum_num_nodes[batch] 得到的是每个点在各自图中的编号\n",
    "    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "    # m = batch * max_num_nodes\n",
    "    # print(index)\n",
    "\n",
    "    dense_x = x.new_full((batch_size * max_num_nodes, ), -60000.0)\n",
    "    # print(dense_x.size())\n",
    "    dense_x[index] = x\n",
    "    # print(x[0:max_num_nodes*3])\n",
    "    # print(dense_x[0:max_num_nodes*3])\n",
    "    dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "    # print(dense_x[:3])\n",
    "    dense_x = softmax(dense_x)\n",
    "    # print(dense_x[-10:])\n",
    "    probs, perm = dense_x.sort(dim=-1, descending=True)\n",
    "    # print(perm)\n",
    "    cum_probs = torch.cumsum(probs, dim=-1)\n",
    "\n",
    "    nucleus = (cum_probs < 0.5)+0\n",
    "    k = torch.count_nonzero(nucleus, dim=1).reshape(-1)\n",
    "    k = torch.clamp(k, min=1)\n",
    "    perm = perm + cum_num_nodes.view(-1, 1)\n",
    "    # print(perm)\n",
    "    perm = perm.view(-1)\n",
    "\n",
    "    index = torch.cat([\n",
    "        torch.arange(k[i], device=x.device) + i * max_num_nodes\n",
    "        for i in range(batch_size)\n",
    "    ], dim=0)\n",
    "\n",
    "    perm = perm[index]\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.5583e-02, 1.6077e-02, 8.1055e-02, 1.5515e-02, 2.3373e-02, 1.5247e-02,\n",
      "         6.0502e-02, 5.6048e-02, 5.8988e-02, 1.1227e-04, 5.0377e-03, 1.2401e-01,\n",
      "         1.2557e-01, 3.4591e-03, 3.1316e-02, 1.5845e-02, 6.2644e-02, 6.0905e-02,\n",
      "         8.3703e-03, 8.4836e-03, 5.3586e-02, 5.7458e-02, 1.3956e-02, 2.5187e-02,\n",
      "         1.2593e-02, 5.8418e-03, 1.7118e-02, 4.0480e-03, 7.6291e-03, 4.4511e-03],\n",
      "        [6.3088e-03, 4.2135e-02, 5.9452e-02, 3.9655e-03, 1.6937e-01, 1.2830e-01,\n",
      "         3.3971e-02, 8.6658e-03, 8.9451e-03, 9.1150e-02, 1.2069e-01, 1.6342e-03,\n",
      "         2.4037e-03, 2.2121e-03, 3.5896e-03, 5.5878e-02, 1.1495e-02, 2.1983e-02,\n",
      "         1.2883e-02, 6.7439e-03, 1.6919e-02, 3.2339e-02, 2.7544e-02, 2.5374e-02,\n",
      "         4.9401e-02, 1.2011e-02, 2.6817e-02, 5.6069e-03, 1.2048e-02, 1.5602e-04],\n",
      "        [7.7877e-02, 1.0929e-02, 1.3849e-02, 7.3203e-02, 8.8657e-03, 1.1420e-02,\n",
      "         2.6735e-02, 9.0363e-02, 3.7434e-02, 2.6756e-02, 8.2849e-03, 2.6076e-02,\n",
      "         3.8743e-02, 8.1002e-02, 5.9114e-02, 4.7571e-03, 7.4130e-02, 5.0500e-02,\n",
      "         2.7238e-03, 5.1525e-02, 3.9292e-02, 3.5179e-02, 2.8812e-03, 5.7691e-02,\n",
      "         8.2646e-03, 5.0958e-02, 1.7672e-02, 5.4462e-03, 4.1604e-03, 4.1682e-03],\n",
      "        [8.4732e-04, 3.4488e-02, 4.9914e-02, 8.1505e-03, 3.6331e-02, 9.9344e-02,\n",
      "         5.8657e-02, 4.5593e-02, 7.2192e-03, 1.3130e-01, 6.5268e-02, 5.4342e-02,\n",
      "         1.2105e-02, 1.0969e-02, 1.8850e-02, 1.1845e-03, 3.9804e-03, 4.9726e-02,\n",
      "         5.0501e-02, 6.2556e-02, 3.8815e-02, 1.2501e-02, 1.7319e-02, 6.4465e-03,\n",
      "         3.2222e-02, 2.2275e-03, 4.0720e-02, 4.2264e-02, 5.6432e-03, 5.1310e-04],\n",
      "        [2.9241e-02, 1.5659e-01, 1.5237e-01, 7.7287e-02, 7.5296e-02, 4.5396e-02,\n",
      "         4.1138e-02, 1.9484e-03, 1.5730e-02, 1.5087e-03, 1.0106e-01, 8.6839e-02,\n",
      "         2.5747e-03, 2.7437e-03, 1.2077e-02, 5.9911e-04, 2.7744e-02, 2.7433e-02,\n",
      "         3.9801e-03, 4.4936e-03, 3.1226e-04, 9.4425e-03, 1.0722e-02, 9.9364e-03,\n",
      "         1.7298e-03, 8.0698e-04, 4.0267e-02, 4.0871e-02, 1.6467e-02, 3.3983e-03],\n",
      "        [5.2925e-02, 5.0508e-02, 4.9609e-02, 5.0316e-03, 5.9653e-03, 1.2597e-01,\n",
      "         5.9629e-02, 4.4509e-02, 8.2137e-03, 1.2892e-02, 1.8310e-02, 3.2990e-02,\n",
      "         3.5593e-02, 9.1280e-03, 3.3218e-03, 9.0846e-02, 7.3891e-02, 3.5395e-02,\n",
      "         1.1292e-02, 4.4548e-02, 6.3707e-02, 1.6874e-02, 8.2846e-03, 3.4912e-02,\n",
      "         2.5347e-02, 3.3011e-03, 8.0103e-03, 2.6113e-02, 2.6503e-02, 1.6382e-02],\n",
      "        [3.4027e-04, 1.2502e-02, 1.8137e-02, 5.8844e-03, 1.3435e-01, 1.2582e-01,\n",
      "         3.5181e-03, 1.3465e-02, 7.0698e-03, 5.2300e-03, 6.7454e-04, 1.6998e-02,\n",
      "         1.7893e-02, 2.5450e-02, 2.0290e-02, 2.6407e-03, 7.7871e-02, 7.9432e-02,\n",
      "         8.8117e-02, 9.2754e-02, 3.7813e-03, 1.4111e-03, 7.0793e-03, 8.7414e-02,\n",
      "         9.1985e-02, 2.3755e-02, 2.1236e-02, 3.1785e-03, 1.1109e-02, 6.1965e-04],\n",
      "        [1.1723e-02, 7.0323e-04, 1.6526e-01, 1.7496e-01, 8.6162e-03, 1.6613e-02,\n",
      "         1.9265e-02, 5.0637e-02, 6.0628e-02, 1.5276e-02, 5.5932e-03, 4.0120e-03,\n",
      "         7.0835e-02, 3.4776e-02, 2.8023e-02, 6.3627e-02, 4.3510e-02, 3.2906e-02,\n",
      "         1.2942e-02, 1.3857e-02, 1.7726e-02, 1.2490e-02, 7.3800e-03, 1.3325e-02,\n",
      "         8.8078e-03, 2.1732e-02, 7.5781e-03, 3.9318e-02, 1.7160e-02, 2.0727e-02],\n",
      "        [1.3753e-01, 1.5671e-01, 3.3212e-02, 6.4489e-02, 9.5769e-03, 4.4057e-02,\n",
      "         5.2230e-02, 2.3253e-02, 1.0830e-03, 1.4196e-02, 4.0424e-03, 6.2132e-02,\n",
      "         6.6591e-02, 7.0093e-03, 3.5736e-02, 3.1563e-02, 6.4246e-03, 9.0513e-03,\n",
      "         3.3277e-02, 3.6653e-02, 5.1200e-03, 6.0607e-03, 3.6529e-02, 4.4286e-02,\n",
      "         7.0099e-03, 2.8304e-02, 3.4606e-02, 7.1450e-04, 3.5794e-03, 4.9700e-03],\n",
      "        [8.9261e-03, 1.7006e-02, 3.8765e-02, 4.5259e-02, 9.1685e-02, 1.1031e-01,\n",
      "         2.3240e-02, 3.8466e-04, 4.1022e-02, 6.5116e-02, 2.6966e-02, 2.5702e-02,\n",
      "         1.0311e-01, 8.3700e-02, 1.4847e-02, 1.2435e-02, 9.7999e-03, 1.7199e-02,\n",
      "         3.5089e-02, 3.7565e-02, 2.4819e-02, 1.5863e-02, 3.5542e-02, 4.2130e-02,\n",
      "         3.2291e-02, 9.3831e-03, 1.4820e-02, 5.3412e-03, 9.7489e-03, 1.9317e-03],\n",
      "        [5.3267e-02, 7.9329e-02, 7.6846e-02, 8.9547e-02, 2.5989e-02, 3.6784e-02,\n",
      "         2.8705e-02, 2.0184e-02, 6.9878e-02, 1.0081e-01, 3.4269e-02, 1.2348e-02,\n",
      "         1.0610e-02, 3.2088e-02, 2.0584e-02, 5.7688e-03, 9.8634e-03, 1.7129e-02,\n",
      "         2.2884e-03, 1.6134e-02, 1.5506e-02, 1.2112e-02, 5.6063e-02, 5.7445e-03,\n",
      "         5.4853e-02, 2.8372e-02, 5.4133e-02, 7.2008e-03, 7.0972e-03, 1.6497e-02],\n",
      "        [1.6876e-01, 2.5250e-02, 2.0871e-02, 1.0281e-02, 1.1890e-02, 3.7076e-02,\n",
      "         6.4841e-02, 1.1809e-03, 1.1965e-03, 1.9337e-02, 1.9980e-02, 4.4132e-02,\n",
      "         7.1758e-02, 5.2803e-02, 9.5192e-02, 5.2793e-02, 5.7025e-02, 1.6377e-02,\n",
      "         1.1964e-02, 2.5163e-02, 1.2547e-03, 1.2338e-02, 8.6848e-03, 8.5306e-03,\n",
      "         3.3185e-02, 5.3808e-02, 1.6861e-02, 3.9488e-02, 9.1721e-03, 8.8023e-03],\n",
      "        [6.5853e-02, 8.5772e-03, 6.7564e-02, 5.9310e-02, 6.2314e-04, 2.1023e-02,\n",
      "         3.9375e-02, 1.2807e-04, 2.9979e-02, 3.6134e-02, 6.4155e-02, 9.4484e-03,\n",
      "         3.1127e-03, 3.3839e-03, 4.8672e-02, 4.6549e-02, 4.0592e-03, 4.8812e-03,\n",
      "         4.6120e-02, 3.9452e-02, 1.1537e-02, 2.8346e-02, 2.8803e-02, 2.7226e-02,\n",
      "         3.5501e-02, 3.5802e-03, 1.2261e-01, 1.2357e-01, 1.1995e-02, 8.4386e-03],\n",
      "        [1.9717e-02, 3.8049e-02, 4.4499e-02, 6.3644e-02, 4.5920e-02, 1.6339e-02,\n",
      "         1.7690e-02, 1.3539e-02, 1.0444e-02, 5.1242e-02, 7.7815e-02, 5.9904e-02,\n",
      "         1.2968e-04, 5.4683e-03, 3.0550e-02, 2.2368e-02, 2.5881e-02, 8.6559e-02,\n",
      "         3.3095e-02, 4.6065e-02, 5.6438e-03, 4.6216e-03, 1.5253e-02, 1.1637e-02,\n",
      "         1.3141e-02, 5.2880e-04, 3.0931e-02, 9.6905e-02, 1.0055e-01, 1.1871e-02],\n",
      "        [2.5342e-02, 8.0278e-02, 4.1242e-02, 4.7005e-02, 1.8939e-02, 3.1811e-02,\n",
      "         5.6242e-03, 1.8890e-02, 8.8033e-02, 7.7839e-02, 2.1832e-02, 6.6083e-02,\n",
      "         3.7150e-02, 8.6190e-02, 9.0848e-02, 8.8988e-03, 4.3559e-03, 4.7070e-03,\n",
      "         5.7011e-03, 9.3780e-03, 7.3215e-03, 3.2416e-03, 8.1008e-02, 7.7424e-02,\n",
      "         6.4462e-03, 8.5266e-03, 9.7017e-04, 2.4326e-02, 1.2816e-02, 7.7743e-03],\n",
      "        [7.9142e-02, 1.6253e-01, 1.7856e-02, 7.4840e-02, 1.1101e-01, 9.2895e-03,\n",
      "         1.9859e-02, 4.6927e-02, 4.0256e-02, 1.5534e-02, 2.7050e-02, 1.6857e-02,\n",
      "         3.5966e-04, 1.2888e-03, 1.8442e-02, 2.8183e-02, 6.0557e-03, 2.9547e-02,\n",
      "         3.2695e-03, 8.5237e-02, 8.6443e-02, 1.5690e-03, 3.1230e-03, 1.8073e-03,\n",
      "         2.4178e-03, 2.6079e-02, 1.1037e-02, 2.9097e-02, 6.7011e-03, 3.8195e-02],\n",
      "        [1.7747e-02, 5.5746e-02, 2.9493e-02, 5.0163e-02, 1.1970e-02, 4.5565e-02,\n",
      "         9.0107e-02, 1.3675e-01, 8.0475e-03, 5.0422e-03, 2.3012e-02, 5.3926e-02,\n",
      "         1.9087e-02, 3.5693e-02, 2.8544e-02, 3.9532e-02, 3.2215e-02, 4.1217e-03,\n",
      "         2.6793e-02, 2.0782e-02, 5.5346e-02, 3.7897e-02, 3.3248e-02, 1.5559e-02,\n",
      "         7.3071e-04, 5.0792e-02, 5.2313e-02, 2.4588e-03, 4.8806e-03, 1.2439e-02],\n",
      "        [3.7287e-03, 3.6479e-02, 2.2500e-02, 5.3161e-02, 1.0747e-02, 1.2095e-03,\n",
      "         5.8575e-02, 3.3724e-02, 2.6984e-03, 3.6249e-02, 6.2753e-02, 5.5690e-02,\n",
      "         5.4155e-03, 3.4500e-02, 9.3726e-02, 1.6946e-02, 1.3562e-01, 3.4802e-03,\n",
      "         7.0213e-02, 5.9511e-02, 2.6178e-02, 2.5866e-02, 1.3779e-02, 3.2817e-02,\n",
      "         3.1510e-02, 1.3983e-02, 6.5972e-03, 1.4860e-02, 1.9268e-02, 1.8213e-02],\n",
      "        [1.4365e-02, 1.1556e-02, 1.4217e-02, 2.4435e-02, 3.2172e-02, 4.9947e-02,\n",
      "         1.1809e-02, 7.0923e-02, 1.4531e-01, 1.7986e-02, 7.8887e-02, 5.9965e-02,\n",
      "         8.2715e-03, 3.6119e-02, 8.4229e-02, 9.3355e-02, 1.2659e-02, 2.6060e-02,\n",
      "         1.9297e-02, 7.8542e-03, 2.8880e-02, 1.0763e-03, 2.9132e-03, 1.5651e-02,\n",
      "         2.8894e-02, 5.1923e-02, 3.3831e-02, 1.9537e-04, 1.6275e-02, 9.4414e-04],\n",
      "        [5.8131e-03, 1.1103e-01, 1.0162e-01, 1.1074e-02, 9.1249e-02, 6.7432e-02,\n",
      "         7.9620e-02, 9.7249e-03, 3.2090e-02, 1.6236e-02, 1.5346e-02, 2.4471e-02,\n",
      "         5.8187e-03, 3.3967e-03, 3.6574e-02, 2.9219e-02, 2.4270e-02, 9.7649e-03,\n",
      "         2.1312e-02, 1.5390e-04, 1.4588e-02, 6.4297e-03, 8.4345e-02, 6.3060e-02,\n",
      "         2.2495e-02, 1.5640e-02, 6.3479e-03, 2.6601e-02, 3.8951e-02, 2.5333e-02]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1]])\n",
      "tensor([22, 25, 24, 25, 27, 25, 25, 26, 24, 24, 26, 26, 28, 29, 24, 26, 26, 25,\n",
      "        26, 26])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "x = torch.rand(20, 32)\n",
    "probs, indices = x.sort(dim=-1, descending=True)\n",
    "# print(x)\n",
    "# print(probs)\n",
    "sm = softmax(probs)\n",
    "# print(sm)\n",
    "grad = sm[:, 1:] - sm[:, :-1]\n",
    "grad = grad[:, 1:] - grad[:, :-1]\n",
    "# print(grad)\n",
    "only_pos = torch.abs(grad)\n",
    "sum = torch.sum(only_pos, dim=1).view(-1, 1)\n",
    "sec_weights = only_pos/sum\n",
    "print(sec_weights)\n",
    "cum_weights = (torch.cumsum(sec_weights, dim=1) > 0.9)+0\n",
    "print(cum_weights)\n",
    "tail_ids = torch.argmax(cum_weights, dim=1)+1\n",
    "tail_ids = tail_ids\n",
    "print(tail_ids)\n",
    "logits = torch.arange(x.size(0))\n",
    "logits_inds = torch.stack((logits, tail_ids), dim=1)\n",
    "# print(logits_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
